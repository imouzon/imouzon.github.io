---
layout: post
title: "Principal Components and Data Mining"
description: "Principal components are a very sketchy member of the data mining tool set."
tags: [principal components, data mining]
image:
  feature: abstract-6.jpg
  credit: dargadgetz
  creditlink: http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/
comments: true
share: true
---





One of the common approaches used when working with big data is 
reducing the number of features through a principal component analysis.
Since it is an unsupervised method though, it may not be a great choice in many 
circumstances. Consider the simple example below





{% highlight r %}
# select the rows and columns
n <- 1000
p <- 100

# generate X
X <- matrix(rnorm(n * p), nrow = n)

# random correlation matrix
R <- randomCovMat(p)

# Give the features some correlation structure
X <- data.frame(X %*% R)

# name the columns
names(X) <- paste0("x", 1:p)

# add the response to our data
X$y <- as.factor(1 * (X$x1 < 0 & X$x2 < 0) | (X$x1 > 
    0 & X$x2 > 0))
X$y <- as.factor(1 * (X$x1 < 0.8 * X$x2))
{% endhighlight %}

This creates a data set with 100 feature dimensions and a binary classification.
However, we create the data so that there is a simple relationship between the
classes and the first two features, x1 and x2.

<!-- class: R plot (results in document) -->

{% highlight r %}
   qplot(x1,x2,color=y,data=X)
{% endhighlight %}

<img src="../images/10-29-thoughts-class1.png" title="plot of chunk class" alt="plot of chunk class" style="display: block; margin: auto;" />

{% highlight r %}
   qplot(x1,x100,color=y,data=X)
{% endhighlight %}

<img src="../images/10-29-thoughts-class2.png" title="plot of chunk class" alt="plot of chunk class" style="display: block; margin: auto;" />


However, with 98 other features to search through, it may be difficult to
identify this relationship immediately, and initially working with this data
we may hope to reduce the data. 

##Principal Components for data reduction

One popular method for data reduction is principal components. 
Consider a data set with each of p features arranged as columns 
of a matrix (giving us an n by p matrix, where n is the number of obsercations the
features are generated by).  We may wish to The basic assumption is 
that for a feature matrix of dimension n by p we can make a set of features 
that the data is and we could build a simple model to capture this behavior:

You may have noticed the data in this case is not comple
<!-- plot: R plot (results in document) -->

{% highlight r %}
   Xmat = as.matrix(X[,1:p])
   rm=rowMeans(Xmat)
   Xnew= Xmat - matrix(rep(rm, ncol(Xmat)), nrow=nrow(Xmat))

   A = Xnew%*%t(Xnew)
   E = eigen(A,TRUE)
   P=t(E$vectors)
   P.X = data.frame(P%*%Xnew)
   names(P.X) = paste0('PC',1:p)
   P.X$y = X$y
{% endhighlight %}

<!-- http://psych.colorado.edu/wiki/lib/exe/fetch.php?media=labs:learnr:emily_-_principal_components_analysis_in_r:pca_how_to.pdf -->

<!-- svm: R code (No Results in Document) -->

{% highlight r %}
Xsvm <- svm(y ~ ., data = X[1:800, ])
prediction <- predict(Xsvm, X[801:1000, ])
table(pred = prediction, true = X$y[801:1000])
{% endhighlight %}



{% highlight text %}
    true
pred  0  1
   0 73 13
   1 23 91
{% endhighlight %}



{% highlight r %}
Xsvm <- svm(y ~ ., data = X[1:800, ])
P.Xsvm <- svm(y ~ ., data = P.X[1:800, ])
prediction <- predict(P.Xsvm, P.X[801:1000, ])
table(pred = prediction, true = P.X$y[801:1000])
{% endhighlight %}



{% highlight text %}
    true
pred   0   1
   0   0   0
   1  96 104
{% endhighlight %}


<!-- randomForest: R code (No Results in Document) -->

{% highlight r %}
Xrf <- randomForest(y ~ ., data = X[1:800, ], ntree = 1000, 
    nodesize = 9, mtry = 10)
prediction <- predict(Xrf, X[801:1000, ])
table(pred = prediction, true = X$y[801:1000])
{% endhighlight %}



{% highlight text %}
    true
pred  0  1
   0 79  9
   1 17 95
{% endhighlight %}



{% highlight r %}
head(importance(Xrf))
{% endhighlight %}



{% highlight text %}
   MeanDecreaseGini
x1           33.016
x2           55.655
x3            3.664
x4            2.838
x5            4.667
x6            1.915
{% endhighlight %}


<!-- randomForest: R code (No Results in Document) -->

{% highlight r %}
P.Xrf <- randomForest(y ~ ., data = P.X[1:800, ], ntree = 1000, 
    nodesize = 9, mtry = 10)
prediction <- predict(P.Xrf, P.X[801:1000, ])
table(pred = prediction, true = P.X$y[801:1000])
{% endhighlight %}



{% highlight text %}
    true
pred  0  1
   0 46 49
   1 50 55
{% endhighlight %}



{% highlight r %}
head(importance(P.Xrf))
{% endhighlight %}



{% highlight text %}
    MeanDecreaseGini
PC1            2.671
PC2            2.849
PC3            2.763
PC4            2.901
PC5            2.741
PC6            2.893
{% endhighlight %}










---
layout: post
title: "Principal Components and Data Mining"
description: "Principal components are a very sketchy member of the data mining tool set."
tags: [principal components, data mining]
image:
  feature: abstract-6.jpg
  credit: dargadgetz
  creditlink: http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/
comments: true
share: true
---


One of the common approaches used when working with big data is 
reducing the number of features through a principal component analysis.
Since it is an unsupervised method though, it may not be a great choice in many 
circumstances. Consider the simple example below


```r
source("~/github/imouzon.github.io/Rmarkdown/code_bank/knitrSetup.r")
require(MASS)
```

```
Loading required package: MASS
```

```r
require(ggplot2)
```

```
Loading required package: ggplot2
```

```r
require(plyr)
```

```
Loading required package: plyr
```

```r
require(reshape2)
```

```
Loading required package: reshape2
```

```r
library(e1071)
```

```
Loading required package: class
```

```r
library(randomForest)
```

```
randomForest 4.6-7
Type rfNews() to see new features/changes/bug fixes.
```

```r
library(mvtnorm)

randomCovMat = function(p) {
    k = 10
    R = matrix(runif(p * p), nrow = p)
    R = (R * lower.tri(R) + t(R * lower.tri(R)))
    diag(R) = sort(sample(1:500)[1:p])
    return(R)
}
```



```r
# select the rows and columns
n = 1000
p = 100

# generate X
X = matrix(rnorm(n * p), nrow = n)

# random correlation matrix
R = randomCovMat(p)

# Give the features some correlation structure
X = data.frame(X %*% R)

# name the columns
names(X) = paste0("x", 1:p)

# add the response to our data
X$y = as.factor(1 * (X$x1 < 0 & X$x2 < 0) | (X$x1 > 0 & X$x2 > 0))
X$y = as.factor(1 * (X$x1 < 0.8 * X$x2))
```

This creates a data set with 100 feature dimensions and a binary classification.
However, we create the data so that there is a simple relationship between the
classes and the first two features, x1 and x2.

<!-- class: R plot (results in document) -->

```r
   qplot(x1,x2,color=y,data=X)
```

![plot of chunk class](figure/class1.png) 

```r
   qplot(x1,x100,color=y,data=X)
```

![plot of chunk class](figure/class2.png) 


However, with 98 other features to search through, it may be difficult to
identify this relationship immediately, and initially working with this data
we may hope to reduce the data. 

##Principal Components for data reduction

One popular method for data reduction is principal components. 
Consider a data set with each of p features arranged as columns 
of a matrix (giving us an n by p matrix, where n is the number of obsercations the
features are generated by).  We may wish to The basic assumption is 
that for a feature matrix of dimension n by p we can make a set of features 
that the data is and we could build a simple model to capture this behavior:

You may have noticed the data in this case is not comple
<!-- plot: R plot (results in document) -->

```r
   Xmat = as.matrix(X[,1:p])
   rm=rowMeans(Xmat)
   Xnew= Xmat - matrix(rep(rm, ncol(Xmat)), nrow=nrow(Xmat))

   A = Xnew%*%t(Xnew)
   E = eigen(A,TRUE)
   P=t(E$vectors)
   P.X = data.frame(P%*%Xnew)
   names(P.X) = paste0('PC',1:p)
   P.X$y = X$y
```

<!-- http://psych.colorado.edu/wiki/lib/exe/fetch.php?media=labs:learnr:emily_-_principal_components_analysis_in_r:pca_how_to.pdf -->

<!-- svm: R code (No Results in Document) -->

```r
Xsvm = svm(y ~ ., data = X[1:800, ])
prediction = predict(Xsvm, X[801:1000, ])
table(pred = prediction, true = X$y[801:1000])
```

```
    true
pred  0  1
   0 72 26
   1 18 84
```



```r
Xsvm = svm(y ~ ., data = X[1:800, ])
P.Xsvm = svm(y ~ ., data = P.X[1:800, ])
prediction = predict(P.Xsvm, P.X[801:1000, ])
table(pred = prediction, true = P.X$y[801:1000])
```

```
    true
pred   0   1
   0  90 110
   1   0   0
```


<!-- randomForest: R code (No Results in Document) -->

```r
Xrf = randomForest(y ~ ., data = X[1:800, ], ntree = 1000, nodesize = 9, mtry = 10)
prediction = predict(Xrf, X[801:1000, ])
table(pred = prediction, true = X$y[801:1000])
```

```
    true
pred  0  1
   0 83 19
   1  7 91
```

```r
head(importance(Xrf))
```

```
   MeanDecreaseGini
x1           42.177
x2           44.217
x3            4.300
x4            2.748
x5            3.527
x6            2.172
```


<!-- randomForest: R code (No Results in Document) -->

```r
P.Xrf = randomForest(y ~ ., data = P.X[1:800, ], ntree = 1000, nodesize = 9, 
    mtry = 10)
prediction = predict(P.Xrf, P.X[801:1000, ])
table(pred = prediction, true = P.X$y[801:1000])
```

```
    true
pred  0  1
   0 65 71
   1 25 39
```

```r
head(importance(P.Xrf))
```

```
    MeanDecreaseGini
PC1            2.947
PC2            3.192
PC3            2.923
PC4            4.000
PC5            3.382
PC6            3.305
```









